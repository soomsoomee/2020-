{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rc\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "import string\n",
    "from pykospacing import spacing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import lightgbm\n",
    "from lightgbm import LGBMRegressor, Dataset, LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Define MAPE formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(actual, pred): \n",
    "    return np.mean(np.abs((actual - pred) / actual)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Load Big Contest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = pd.read_excel('./data/01_제공데이터/2020 빅콘테스트 데이터분석분야-챔피언리그_2019년 실적데이터_v1_200818.xlsx', skiprows=1)\n",
    "testset = pd.read_excel('./data/02_평가데이터/2020 빅콘테스트 데이터분석분야-챔피언리그_2020년 6월 판매실적예측데이터(평가데이터).xlsx', skiprows=1)\n",
    "watching_rate_data = pd.read_excel('./data/01_제공데이터/2020 빅콘테스트 데이터분석분야-챔피언리그_시청률 데이터.xlsx', skiprows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Load external data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv('./data/external_data/weather/weather_df.csv').iloc[:,1:]\n",
    "munzi = pd.read_csv('./data/external_data/weather/munzi_pm10_utf.csv')\n",
    "brand_df = pd.read_excel('./data/external_data/product_info/brand.xlsx').iloc[:,1:]\n",
    "trend_df = pd.read_csv('./data/external_data/product_info/trend검색량.csv')\n",
    "trend_dict = pd.read_excel('./data/external_data/product_info/trend_dict.xlsx')\n",
    "search_df = pd.read_csv('./data/external_data/product_info/상품검색량.csv')\n",
    "search_dict = pd.read_excel('./data/external_data/product_info/search_dict.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) for basic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df, train=True) :\n",
    "\n",
    "    # -- Column명 영어로 바꾸기\n",
    "    df.columns = ['datetime', 'duration', 'mcode', 'pcode', 'pname', 'category', 'price', 'sales']\n",
    "    \n",
    "    # -- 노출(분)의 결측치를 앞의 값으로 채우기\n",
    "    df['duration'].fillna(method='ffill', inplace=True)\n",
    "    \n",
    "    if train :\n",
    "        # -- sales_0 : 원래 취급액이 0이었는지의 여부를 나타내는 칼럼\n",
    "        df['sales_0'] = [0 if pd.notna(i) else 1 for i in df.sales]\n",
    "        # 취급액이 0이었으면, 해당 상품 가격으로 치환 / 그렇지 않으면 취급액 그대로 둠\n",
    "        df['sales'] = [s if s0 == 0 else p for s0, s, p in df[['sales_0', 'sales', 'price']].values]        \n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) to split datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_split(df, train=True) :\n",
    "    year = []; month = []; day= []; hour = []; minute = []; weekday = [];\n",
    "    days = ['mon', 'tue', 'wed', 'thurs', 'fri', 'sat', 'sun']\n",
    "    for i in df.datetime :\n",
    "        year.append(i.year)\n",
    "        month.append(i.month)\n",
    "        day.append(i.day)\n",
    "        hour.append(i.hour)\n",
    "        minute.append(i.minute)\n",
    "        weekday.append(days[i.weekday()])\n",
    "\n",
    "    for k, v in {'year':year, 'month':month, 'day':day, 'hour':hour, 'minute':minute, 'weekday':weekday}.items() :\n",
    "        df[k] = pd.Series(v)\n",
    "\n",
    "    df['hm'] = pd.Series([str(i).rjust(2, '0')+':'+str(j).rjust(2, '0') for i, j in zip(hour, minute)])\n",
    "\n",
    "    if train :\n",
    "        return df[['datetime', 'year', 'month', 'day', 'hour', 'minute', 'hm', 'weekday', \n",
    "                   'duration', 'mcode', 'pcode', 'pname', 'category', 'price', 'sales', 'sales_0']]\n",
    "    else :\n",
    "        return df[['datetime', 'year', 'month', 'day', 'hour', 'minute', 'hm', 'weekday', \n",
    "                   'duration', 'mcode', 'pcode', 'pname', 'category', 'price', 'sales']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) to make day_order, repeat_order, pgm_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_pgm_repeat_order(df, train=True) :\n",
    "    if train :\n",
    "        date_unique = df.datetime.dt.date.unique().tolist()\n",
    "        trouble_no = [100271, 100270, 100251, 100250, 100272, 100271, 100846, \n",
    "                      100845, 100189, 100190, 100271, 100270, 100036, 100038, \n",
    "                      100039, 100187, 100189]\n",
    "        a = [100270, 100271, 100272]; b = [100250, 100251]\n",
    "        c = [100845, 100846]; d = [100187, 100189, 100190]; e = [100036, 100038, 100039]\n",
    "        \n",
    "        df_ = []; day_od = 1\n",
    "        for i in range(len(date_unique)-1) :\n",
    "            # print(i)\n",
    "            date = date_unique[i]\n",
    "            date_1 = date_unique[i+1]\n",
    "            if i < 304 :\n",
    "                m=0; m_1=0;\n",
    "            elif date == datetime.date(2019, 11, 26) :\n",
    "                m=0; m_1=20;\n",
    "                # 2019년 11월 26일은 유일하게 6:00 방송 시작해서, 2:00에 마지막 방송을 함\n",
    "                # (원래 6:00 방송 시작하면 1:40이 막방)\n",
    "            else :\n",
    "                m=20; m_1=20;\n",
    "            temp = df[df.datetime >= datetime.datetime(\n",
    "                date.year, date.month, date.day, 6, m)]\n",
    "            temp = temp[temp.datetime < datetime.datetime(\n",
    "                date_1.year, date_1.month, date_1.day, 2, m_1)]\n",
    "        \n",
    "            # mcode가 같으면 같은 숫자\n",
    "            cnt=1; mcode_before=0; values_per_day=[]\n",
    "            for i, value in enumerate(temp.values.tolist(), 1) :\n",
    "                if mcode_before == 0 :\n",
    "                    values_per_day.append(value+[cnt, day_od])\n",
    "                    mcode_before = value[9]\n",
    "                elif value[9] in trouble_no:\n",
    "                    one_before = temp.values.tolist()[i-2][9]\n",
    "                    if (one_before not in trouble_no) \\\n",
    "                        or ((value[9] in a) and (one_before not in a)) \\\n",
    "                            or ((value[9] in b) and (one_before not in b)) \\\n",
    "                                or ((value[9] in c) and (one_before not in c)) \\\n",
    "                                    or ((value[9] in d) and (one_before not in d)) \\\n",
    "                                        or ((value[9] in e) and (one_before not in e)) :\n",
    "                            cnt += 1\n",
    "                    values_per_day.append(value+[cnt, day_od])\n",
    "                else :\n",
    "                    if value[9] == mcode_before :\n",
    "                        values_per_day.append(value+[cnt, day_od])\n",
    "                    else :\n",
    "                        cnt += 1\n",
    "                        values_per_day.append(value+[cnt, day_od])\n",
    "                        mcode_before = value[9]\n",
    "            df_+= values_per_day\n",
    "            day_od += 1\n",
    "        df_ = pd.DataFrame(df_, columns=df.columns.tolist()+['pgm_order', 'day_order'])\n",
    "\n",
    "    else :\n",
    "        date_unique = df.datetime.dt.date.unique().tolist()\n",
    "        \n",
    "        a = [100037, 100036, 100038]; b = [100039, 100038, 100037, 100036]\n",
    "        c = [100037, 100039]; d = [100662, 100663]\n",
    "        trouble_no = a+b+c+d\n",
    "        \n",
    "        df_ = []; day_od = 1\n",
    "        for i in range(len(date_unique)-1) :\n",
    "        \n",
    "            date = date_unique[i]\n",
    "            date_1 = date_unique[i+1]\n",
    "        \n",
    "            temp = df[df.datetime >= datetime.datetime(\n",
    "                date.year, date.month, date.day, 6, 0)]\n",
    "            temp = temp[temp.datetime <= datetime.datetime(\n",
    "                date_1.year, date_1.month, date_1.day, 2, 20)]\n",
    "        \n",
    "            # mcode가 같으면 같은 숫자\n",
    "            cnt=1; mcode_before=0; values_per_day=[]\n",
    "            for i, value in enumerate(temp.values.tolist(), 1) :\n",
    "                if mcode_before == 0 :\n",
    "                    values_per_day.append(value+[cnt, day_od])\n",
    "                    mcode_before = value[9]\n",
    "                elif value[9] in trouble_no:\n",
    "                    one_before = temp.values.tolist()[i-2][9]\n",
    "                    if (one_before not in trouble_no) \\\n",
    "                        or ((value[9] in a) and (one_before not in a)) \\\n",
    "                            or ((value[9] in b) and (one_before not in b)) \\\n",
    "                                or ((value[9] in c) and (one_before not in c)) \\\n",
    "                                    or ((value[9] in d) and (one_before not in d)) :\n",
    "                            cnt += 1\n",
    "                    values_per_day.append(value+[cnt, day_od])\n",
    "                else :\n",
    "                    if value[9] == mcode_before :\n",
    "                        values_per_day.append(value+[cnt, day_od])\n",
    "                    else :\n",
    "                        cnt += 1\n",
    "                        values_per_day.append(value+[cnt, day_od])\n",
    "                        mcode_before = value[9]\n",
    "            df_+= values_per_day\n",
    "            day_od += 1\n",
    "        df_ = pd.DataFrame(df_, columns=df.columns.tolist()+['pgm_order', 'day_order'])\n",
    "\n",
    "    repeate_info = df_.groupby(['datetime', 'pgm_order']).count().mcode.reset_index().values.tolist()\n",
    "    repeat_order = []; pgm_before = 0\n",
    "    for i in range(len(repeate_info)) :\n",
    "        if repeate_info[i][1] != pgm_before :\n",
    "            rep = 1\n",
    "        else :\n",
    "            rep += 1\n",
    "        for _ in range(repeate_info[i][2]) :\n",
    "            repeat_order.append(rep)\n",
    "        pgm_before = repeate_info[i][1]\n",
    "    df_['repeat_order'] = pd.Series(repeat_order)     \n",
    "\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) to vertorize product names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pname_token_df(df_, pname_cols=None, pname_vocab=None, train=True) :\n",
    "    \n",
    "    df = df_.copy()\n",
    "    \n",
    "    if train :\n",
    "        \n",
    "        df['pname'] = df['pname'].str.replace(f'[{string.punctuation}]', ' ')\n",
    "        df['pname'] = df['pname'].str.replace('  ', ' ')\n",
    "        df['pname'] = df['pname'].str.strip()\n",
    "        \n",
    "        pname_unique = df.pname.unique().tolist()\n",
    "        pname_spaced = {pn:'' for pn in pname_unique}\n",
    "        for pn in pname_unique :\n",
    "            pname_spaced[pn] = spacing(pn)\n",
    "        \n",
    "        df = df.replace({\"pname\": pname_spaced})\n",
    "        \n",
    "        df['pname'] = df['pname'].str.replace('삼성노트북', '삼성 노트북')\n",
    "        df['pname'] = df['pname'].str.replace('쿠쿠전기밥솥', '쿠쿠 전기밥솥')\n",
    "        df['pname'] = df['pname'].str.replace('한 샘', '한샘')\n",
    "        df['pname'] = df['pname'].str.replace('삼성5도어', '삼성 5도어')\n",
    "        df['pname'] = df['pname'].str.replace('에 버홈', '에버홈')\n",
    "        df['pname'] = df['pname'].str.replace('휴롬퀵스퀴저', '휴롬 퀵스퀴저')\n",
    "        df['pname'] = df['pname'].str.replace('매직 쉐프', '매직쉐프')\n",
    "        df['pname'] = df['pname'].str.replace('쿠진나이프케어', '쿠진 나이프케어')\n",
    "        df['pname'] = df['pname'].str.replace('에 지리', '에지리')\n",
    "        df['pname'] = df['pname'].str.replace('린 나이', '린나이')\n",
    "        df['pname'] = df['pname'].str.replace('한삼인순홍삼진7박스', '한삼인 순홍삼진7박스')\n",
    "        df['pname'] = df['pname'].str.replace('미니건조기', '미니 건조기')\n",
    "        \n",
    "        # => pname bigram\n",
    "        vectorizer = CountVectorizer(min_df=30, ngram_range=(1, 2))\n",
    "        features = vectorizer.fit_transform(df['pname'].values.tolist())\n",
    "        \n",
    "        pname_bigram_df = pd.DataFrame(features.toarray())\n",
    "        pname_bigram_df.columns = ['pname_'+str(i) for i in pname_bigram_df.columns]\n",
    "        \n",
    "        pname_voab = vectorizer.vocabulary_\n",
    "        return pname_bigram_df, pname_voab\n",
    "\n",
    "    else : # testset 만들 때\n",
    "\n",
    "        df['pname'] = df['pname'].str.replace(f'[{string.punctuation}]', ' ')\n",
    "        df['pname'] = df['pname'].str.replace('  ', ' ')\n",
    "        df['pname'] = df['pname'].str.strip()\n",
    "        \n",
    "        pname_unique = df.pname.unique().tolist()\n",
    "        pname_spaced = {pn:'' for pn in pname_unique}\n",
    "        for pn in pname_unique :\n",
    "            pname_spaced[pn] = spacing(pn)\n",
    "        \n",
    "        df = df.replace({\"pname\": pname_spaced})\n",
    "        \n",
    "        df['pname'] = df['pname'].str.replace('삼성노트북', '삼성 노트북')\n",
    "        df['pname'] = df['pname'].str.replace('쿠쿠전기밥솥', '쿠쿠 전기밥솥')\n",
    "        df['pname'] = df['pname'].str.replace('한 샘', '한샘')\n",
    "        df['pname'] = df['pname'].str.replace('삼성5도어', '삼성 5도어')\n",
    "        df['pname'] = df['pname'].str.replace('에 버홈', '에버홈')\n",
    "        df['pname'] = df['pname'].str.replace('휴롬퀵스퀴저', '휴롬 퀵스퀴저')\n",
    "        df['pname'] = df['pname'].str.replace('매직 쉐프', '매직쉐프')\n",
    "        df['pname'] = df['pname'].str.replace('쿠진나이프케어', '쿠진 나이프케어')\n",
    "        df['pname'] = df['pname'].str.replace('에 지리', '에지리')\n",
    "        df['pname'] = df['pname'].str.replace('린 나이', '린나이')\n",
    "        df['pname'] = df['pname'].str.replace('한삼인순홍삼진7박스', '한삼인 순홍삼진7박스')\n",
    "        df['pname'] = df['pname'].str.replace('미니건조기', '미니 건조기')\n",
    "        \n",
    "        # => pname bigram\n",
    "        vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "        features = vectorizer.fit_transform(df['pname'].values.tolist())\n",
    "        testset_pname_vocab = vectorizer.vocabulary_\n",
    "        \n",
    "        pname_idx = []\n",
    "        testset_idx_to_train_idx = dict()\n",
    "        for k, v in testset_pname_vocab.items() :\n",
    "            if k in pname_vocab :\n",
    "                pname_idx.append(testset_pname_vocab[k])\n",
    "                testset_idx_to_train_idx[testset_pname_vocab[k]] = pname_vocab[k]\n",
    "        \n",
    "        temp = pd.DataFrame(features.toarray())\n",
    "        temp = temp.iloc[:,pname_idx]\n",
    "        temp.rename(columns=testset_idx_to_train_idx, inplace=True)\n",
    "        temp.columns = ['pname_'+str(i) for i in temp.columns]\n",
    "        \n",
    "        pname_bigram_df = pd.DataFrame(columns=pname_cols)\n",
    "        pname_bigram_df_cols = temp.columns.tolist()\n",
    "        for col in pname_bigram_df.columns :\n",
    "            if col in pname_bigram_df_cols : \n",
    "                pname_bigram_df[col] = temp[col]\n",
    "            else :\n",
    "                pname_bigram_df[col] = 0\n",
    "        \n",
    "        return pname_bigram_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) to create pseudo mcode/pcode of testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pseudo_m_pcode(df_, pname_bigram_df, test_month_dorders=None, pseudo_mcode_model=None, pseudo_pcode_model=None, \n",
    "                        m_cols=None, p_cols=None, train=True, train_validation=False) :\n",
    "\n",
    "    if train and (not train_validation) : # testset(2020.6)의 유사코드를 부여하기 위해 모델을 생성할 때\n",
    "        \n",
    "        df = df_.copy()\n",
    "        df['brand'] = df['brand'].astype('str')\n",
    "        \n",
    "        train_mcode_df = pd.concat([pd.get_dummies(df['brand']), \n",
    "                                    df[['mcode', 'price']], \n",
    "                                    pname_bigram_df],\n",
    "                                   axis=1)\n",
    "\n",
    "        train_pcode_df = pd.concat([pd.get_dummies(df['brand']), \n",
    "                                    df[['pcode', 'price']], \n",
    "                                    pname_bigram_df],\n",
    "                                   axis=1)\n",
    "        \n",
    "        train_mcodes = train_mcode_df.mcode; del train_mcode_df['mcode']\n",
    "        pseudo_mcode_model = KNeighborsClassifier(n_neighbors=3).fit(train_mcode_df.values, train_mcodes.values)\n",
    "\n",
    "        train_pcodes = train_pcode_df.pcode; del train_pcode_df['pcode']\n",
    "        pseudo_pcode_model = KNeighborsClassifier(n_neighbors=3).fit(train_pcode_df.values, train_pcodes.values) \n",
    "        \n",
    "        m_cols = train_mcode_df.columns.tolist()\n",
    "        p_cols = train_pcode_df.columns.tolist()\n",
    "        \n",
    "        return pseudo_mcode_model, pseudo_pcode_model, m_cols, p_cols, train_mcodes.unique().tolist(), train_pcodes.unique().tolist()\n",
    "\n",
    "    \n",
    "    elif not train : # testset(2020.6)에 유사코드 부여할 때\n",
    "\n",
    "        df = df_.copy()\n",
    "        df['brand'] = df['brand'].astype('str')\n",
    "\n",
    "        mp_code_raw = df[['mcode', 'pcode']]        \n",
    "        test_mcode_df = pd.concat([pd.get_dummies(df['brand']), \n",
    "                                   df[['mcode', 'price']], \n",
    "                                   pname_bigram_df],\n",
    "                                  axis=1).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "        test_pcode_df = pd.concat([pd.get_dummies(df['brand']), \n",
    "                                   df[['pcode', 'price']], \n",
    "                                   pname_bigram_df],\n",
    "                                  axis=1).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "        real_mcodes = test_mcode_df.mcode.values.tolist()\n",
    "        real_pcodes = test_pcode_df.pcode.values.tolist()\n",
    "        \n",
    "        print('testset 구성 중..')\n",
    "        test_mcols = test_mcode_df.columns.tolist()\n",
    "        test_pcols = test_pcode_df.columns.tolist()\n",
    "        for c in m_cols :\n",
    "            if c not in test_mcols :\n",
    "                test_mcode_df[c] = pd.Series()\n",
    "                test_mcode_df[c] = 0\n",
    "        for c in p_cols :\n",
    "            if c not in test_pcols :\n",
    "                test_pcode_df[c] = pd.Series()\n",
    "                test_pcode_df[c] = 0\n",
    "\n",
    "        test_mcode_df = test_mcode_df[m_cols]\n",
    "        test_pcode_df = test_pcode_df[p_cols]\n",
    "\n",
    "        print('유사코드 생성 중..')\n",
    "        pseudo_mcodes = pseudo_mcode_model.predict(test_mcode_df.values)\n",
    "        pseudo_pcodes = pseudo_pcode_model.predict(test_pcode_df.values)\n",
    "        pseudo_mcode_df = pd.DataFrame({'real':real_mcodes, 'pseudo':pseudo_mcodes})\n",
    "        pseudo_pcode_df = pd.DataFrame({'real':real_pcodes, 'pseudo':pseudo_pcodes})\n",
    "                \n",
    "        return df, mp_code_raw, pseudo_mcode_df, pseudo_pcode_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (6) to optimize broadcast programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_for_optimal(trainset, mp_code_raw, month_test, pseudo_pcode_dict, pseudo_mcode_dict,\n",
    "                           tr_cols, search_cols, token_cols, trend_cols, pre_cols,\n",
    "                           day_orders_for_optimize, full_day_orders_for_month, day=True) :\n",
    "\n",
    "    train_subset = trainset.copy()\n",
    "    \n",
    "    # -- 실제 m/pcode로 치환\n",
    "    train_subset['pcode'] = mp_code_raw['pcode']\n",
    "    train_subset['mcode'] = mp_code_raw['mcode']\n",
    "    \n",
    "    # day_order(1~30), pgm_order(1~20), repeat_order(1~3)\n",
    "    time_idx = [[d,p,r] \n",
    "                for d in range(full_day_orders_for_month[0], full_day_orders_for_month[-1]+1) \n",
    "                for p in range(1,21) \n",
    "                for r in range(1,4)]\n",
    "    time_idx = pd.DataFrame(time_idx, columns=['day_order', 'pgm_order', 'repeat_order'])\n",
    "    \n",
    "    get_product_groups = train_subset.groupby(['day_order', 'pgm_order']).nunique().pcode.reset_index()\n",
    "    get_product_groups.columns = ['day_order', 'pgm_order', 'pcode_cnt']\n",
    "    \n",
    "    pgroups_for_day_orders = [] # 상품 그룹\n",
    "    for row in get_product_groups.values :\n",
    "        if day : # 하루 편성 이라면\n",
    "            if row[0] == day_orders_for_optimize :\n",
    "                pgroups_for_day_orders.append(train_subset.loc[(train_subset.day_order == row[0]) \n",
    "                                                               & (train_subset.pgm_order == row[1])\n",
    "                                                                & (trainset.sales_0 == 0),:].pcode.unique().tolist())\n",
    "        else : # 주 단위 편성이라면\n",
    "            if day_orders_for_optimize[0] <= row[0] <= day_orders_for_optimize[-1] :\n",
    "                pgroups_for_day_orders.append(train_subset.loc[(train_subset.day_order == row[0]) \n",
    "                                                               & (train_subset.pgm_order == row[1])\n",
    "                                                                & (trainset.sales_0 == 0),:].pcode.unique().tolist())\n",
    "    \n",
    "    product_groups_unique = list(list(map(lambda i: list(sorted(i)), pgroups_for_day_orders)))    \n",
    "    \n",
    "    if day : # 하루 편성이라면\n",
    "        time_idx_sub = time_idx.loc[(time_idx.day_order == day_orders_for_optimize), :]\n",
    "    else : # 일주일 편성이라면\n",
    "        time_idx_sub = time_idx.loc[(time_idx.day_order >= day_orders_for_optimize[0]) \n",
    "                                    & (time_idx.day_order <= day_orders_for_optimize[-1]), :]\n",
    "    \n",
    "    rows = []\n",
    "    for group in product_groups_unique :\n",
    "        for row in time_idx_sub.values.tolist() :\n",
    "            for i in range(len(group)) :\n",
    "                rows.append(row + [group[i]])\n",
    "    \n",
    "    testset_frame = pd.DataFrame(rows, columns=['day_order', 'pgm_order', 'repeat_order', 'pcode'])\n",
    "    testset_frame = testset_frame.sort_values(by=['day_order', 'pgm_order', 'pcode']).reset_index(drop=True)\n",
    "    \n",
    "    # 중복 제거\n",
    "    testset_frame = testset_frame.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    # -- day_order, pgm_order에 대응되는 시간변수\n",
    "    time_vars = ['month', 'weekday', 'week_order', 'holiday'] + search_cols + trend_cols\n",
    "    time_vars_df = train_subset[['day_order', 'pgm_order']+time_vars].drop_duplicates().reset_index(drop=True)\n",
    "    time_vars_df['pgm_order'] = time_vars_df['pgm_order'].astype('int')\n",
    "    testset_frame['pgm_order'] = testset_frame['pgm_order'].astype('int')\n",
    "    testset_frame = testset_frame.merge(time_vars_df, on=['day_order', 'pgm_order'], how='left')\n",
    "\n",
    "    externals = ['ta', 'rn_or_not', 'ws', 'humid', 'dsnw', 'munzi', 'watching_rate']\n",
    "    train_subset['pgm_order'] = train_subset['pgm_order'].astype('int')\n",
    "    ex_vars_df = train_subset.groupby(['day_order', 'pgm_order']).mean()[externals].reset_index()\n",
    "    testset_frame = testset_frame.merge(ex_vars_df, on=['day_order', 'pgm_order'], how='left')\n",
    "    \n",
    "    # -- pcode에 대응되는 상품변수\n",
    "    product_vars = ['mcode', 'category', 'price', 'high_price', 'low_price', 'price_bin', \n",
    "                    'first_sale', 'sales_0_before', 'gender', 'payment_opt', 'vol_under5', 'vol_under10', 'sales_under100'] + token_cols + pre_cols\n",
    "    if day :\n",
    "        product_vars_df = train_subset.loc[(train_subset.day_order == day_orders_for_optimize) \n",
    "                                           & (trainset.sales_0 == 0), :][['pcode']+product_vars].drop_duplicates().reset_index(drop=True)\n",
    "    else :\n",
    "        product_vars_df = train_subset.loc[(train_subset.day_order >= day_orders_for_optimize[0]) \n",
    "                                           & (train_subset.day_order <= day_orders_for_optimize[-1]) \n",
    "                                           & (trainset.sales_0 == 0), :][['pcode']+product_vars].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    product_vars_df = product_vars_df.groupby('pcode').last().reset_index()\n",
    "    product_vars_df['pcode'] = product_vars_df['pcode'].astype('int')\n",
    "    testset_frame['pcode'] = testset_frame['pcode'].astype('int')\n",
    "    testset_frame = testset_frame.merge(product_vars_df, on=['pcode'], how='left')  \n",
    "\n",
    "    # duration=20\n",
    "    testset_frame['duration'] = 20\n",
    "    \n",
    "    # p_cnt\n",
    "    p_cnt_dict = {}\n",
    "    for group in product_groups_unique :\n",
    "        for item in group :\n",
    "            p_cnt_dict[item] = len(group)\n",
    "    testset_frame = testset_frame.merge(pd.DataFrame({'pcode':list(p_cnt_dict.keys()), \n",
    "                                                      'p_cnt':list(p_cnt_dict.values())}), on='pcode')\n",
    "    \n",
    "    # accumed_cnt\n",
    "    acc_cnt = {}\n",
    "    if day : # 하루 편성이라면\n",
    "        temp = train_subset.loc[(train_subset.day_order == day_orders_for_optimize) \n",
    "                                & (trainset.sales_0 == 0),:][['pcode', 'accumed_cnt']]\n",
    "    else : # 일주일 편성이라면\n",
    "        temp = train_subset.loc[(train_subset.day_order >= day_orders_for_optimize[0]) \n",
    "                                & (train_subset.day_order <= day_orders_for_optimize[-1]) \n",
    "                                & (trainset.sales_0 == 0), :][['pcode', 'accumed_cnt']]\n",
    "\n",
    "    for p in list(p_cnt_dict.keys()) :\n",
    "        v = temp[temp.pcode == str(p)].accumed_cnt.values.tolist()\n",
    "        if v :\n",
    "            acc_cnt[p] = v[-1]\n",
    "        else :\n",
    "            acc_cnt[p] = 1\n",
    "    testset_frame = testset_frame.merge(pd.DataFrame({'pcode':list(acc_cnt.keys()), 'accumed_cnt':list(acc_cnt.values())}), on='pcode')\n",
    "    frame = testset_frame.copy()\n",
    "    \n",
    "    group_idx = {i:v for i, v in enumerate(product_groups_unique, 1)}\n",
    "    \n",
    "    testset_frame['pcode'] = testset_frame['pcode'].replace(pseudo_pcode_dict)\n",
    "    testset_frame['mcode'] = testset_frame['mcode'].replace(pseudo_mcode_dict)\n",
    "\n",
    "    \n",
    "    # 사용할 변수 정의\n",
    "    dummy_cols = ['month', 'weekday', 'duration', 'category', 'gender', 'payment_opt',\n",
    "                  'pgm_order', 'repeat_order', 'week_order', 'price_bin', 'high_price', 'low_price', 'mcode', 'pcode']\n",
    "    numeric_cols = ['p_cnt', 'price', 'accumed_cnt']\n",
    "    binary_cols = ['holiday', 'first_sale', 'sales_0_before', 'vol_under5', 'vol_under10', 'sales_under100']\n",
    "    pre_cols = ['pre_volume_median', 'pre_volume_max', 'pre_volume_min', 'pre_volume_var',\n",
    "                'pre_sales_median', 'pre_sales_max', 'pre_sales_min', 'pre_sales_var']\n",
    "    weather_cols = ['ta', 'rn_or_not', 'ws', 'humid', 'dsnw', 'munzi']\n",
    "    w_rate = ['watching_rate']\n",
    "    \n",
    "    testset_frame['week_order'] = testset_frame['week_order'].astype('int')\n",
    "    testset_frame['month'] = testset_frame['month'].astype('int') \n",
    "    testset_frame['duration'] = testset_frame['duration'].astype('float') \n",
    "    \n",
    "    # 변수 데이터셋 생성\n",
    "    df = pd.DataFrame()\n",
    "    for col in dummy_cols :\n",
    "        df[col] = pd.Series()\n",
    "        df[col] = testset_frame[col].astype('str')\n",
    "        df[col] = df[col].astype('str')\n",
    "    df_dummies = pd.get_dummies(df[dummy_cols])\n",
    "    df_numeric = testset_frame[numeric_cols + weather_cols + w_rate + token_cols + pre_cols + search_cols + trend_cols]\n",
    "\n",
    "    features = pd.concat([df_dummies,\n",
    "                          testset_frame[binary_cols], \n",
    "                          df_numeric\n",
    "                         ],\n",
    "                         axis=1)    \n",
    "\n",
    "    features_ = pd.DataFrame(columns=tr_cols)\n",
    "    test_cols = features.columns.tolist()\n",
    "    for c in tr_cols :\n",
    "        if c in test_cols :\n",
    "            features_[c] = features[c]\n",
    "        else :\n",
    "            features_[c] = pd.Series()\n",
    "            features_[c] = 0\n",
    "\n",
    "    cols = ['month_'+str(i) for i in range(1,13) if i != month_test]\n",
    "    for c in cols :\n",
    "        features_[c] = features_[c].fillna(0).astype('int')\n",
    "\n",
    "    return features_, frame, group_idx, pgroups_for_day_orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - preprocessing(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = preprocessing(trainset, train=True)\n",
    "trainset = date_split(trainset, train=True)\n",
    "trainset = day_pgm_repeat_order(trainset, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - preprocessing(2) + make variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 무형상품 제거\n",
    "trainset = trainset[trainset.category != '무형'].reset_index(drop=True)\n",
    "\n",
    "# EDA를 위한 판매량 칼럼 추가\n",
    "trainset['sales_volume'] = trainset.sales / trainset.price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - sales_0 pcode list (취급액이 0인 적이 있는 상품 리스트)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_0_df = trainset[trainset.sales_0 == 1][['pcode', 'pname']].drop_duplicates().reset_index(drop=True)\n",
    "sales_0_pcodes = sales_0_df['pcode'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAR 1) week_order\n",
    "주(week)차 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_order = []\n",
    "for i in trainset.day :\n",
    "    if 1 <= i <= 7 :\n",
    "        week_order.append(1)\n",
    "    elif 8 <= i <= 15 :\n",
    "        week_order.append(2)\n",
    "    elif 16 <= i <= 23 :\n",
    "        week_order.append(3)\n",
    "    else :\n",
    "        week_order.append(4)\n",
    "trainset['week_order'] = pd.Series(week_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAR 2) holiday\n",
    "휴일 여부 / 공휴일 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday = []\n",
    "for i, j in zip(trainset.datetime.dt.date, trainset.weekday) :\n",
    "    if j in ('sat', 'sun') :\n",
    "        holiday.append(1)\n",
    "    elif str(i) in ('2019-01-01', '2019-02-04', '2019-02-05', '2019-02-06', \n",
    "                    '2019-03-01', '2019-05-06', '2019-05-12', '2019-06-06', \n",
    "                    '2019-08-15', '2019-09-12', '2019-09-13', '2019-09-14', \n",
    "                    '2019-10-03', '2019-10-09', '2019-12-25') :\n",
    "        holiday.append(1)\n",
    "    else :\n",
    "        holiday.append(0)\n",
    "trainset['holiday'] = pd.Series(holiday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAR 3) high price / low price\n",
    "동일 방송에서 여러개의 상품을 판매하고 그 상품의 가격들이 다를 때, 가장 비싼 상품인지 / 가장 저렴한 상품인지의 여부\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_nunique = trainset.groupby(['day_order', 'pgm_order']).nunique().price.reset_index()\n",
    "price_nunique = price_nunique[price_nunique.price > 1].reset_index(drop=True)\n",
    "\n",
    "use_nan = None; high_price = []; low_price = []\n",
    "cols = trainset.columns.tolist()\n",
    "for row in trainset.values :\n",
    "    if price_nunique.loc[(price_nunique.day_order == row[cols.index('day_order')]) & (price_nunique.pgm_order == row[cols.index('pgm_order')]), :].shape[0] :\n",
    "        temp = trainset.loc[(trainset.day_order == row[cols.index('day_order')]) & (trainset.pgm_order == row[cols.index('pgm_order')]), :]\n",
    "        if row[cols.index('price')] == temp.price.max() :\n",
    "            high_price.append(1)\n",
    "        else :\n",
    "            high_price.append(0)\n",
    "        if row[cols.index('price')] == temp.price.min() :\n",
    "            low_price.append(1)\n",
    "        else :\n",
    "            low_price.append(0)            \n",
    "    else :\n",
    "        high_price.append(2) # 해당 없음\n",
    "        low_price.append(2) # 해당 없음\n",
    "\n",
    "trainset['high_price'] = pd.Series(high_price)\n",
    "trainset['low_price'] = pd.Series(low_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAR 4) price_bin (가격 구간화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcut_idx_df = pd.DataFrame({'qcut':pd.qcut(trainset.price, 10), \n",
    "                            'label':LabelEncoder().fit_transform(pd.qcut(trainset.price, 10))}).drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset['price_bin'] = LabelEncoder().fit_transform(pd.qcut(trainset.price, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAR 5) p_cnt (함께 판매된 상품의 개수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = trainset.groupby('datetime').count().pcode.reset_index()\n",
    "temp_df.columns = ['datetime', 'p_cnt']\n",
    "trainset = trainset.merge(temp_df, on=['datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAR 6) accumed_cnt/fist_sale (누적 방송 횟수/첫 방송 여부)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcode_per_day = trainset.groupby(['day_order', 'pgm_order', 'pcode']).count().reset_index()[['day_order', 'pgm_order', 'pcode']]\n",
    "\n",
    "cum_sale = []; first_sale=[]; no = 1\n",
    "for i in pcode_per_day.values :\n",
    "    cnt = 1\n",
    "    temp = pcode_per_day.loc[(pcode_per_day.day_order < i[0]) & (pcode_per_day.pcode == i[2]), :].values\n",
    "    check = temp.tolist()\n",
    "    if check :\n",
    "        first_sale.append(0)\n",
    "        cum_sale.append(len(temp))\n",
    "    else :\n",
    "        first_sale.append(1)\n",
    "        cum_sale.append(0)\n",
    "\n",
    "pcode_per_day['accumed_cnt'] = pd.Series(cum_sale)\n",
    "pcode_per_day['first_sale'] = pd.Series(first_sale)\n",
    "\n",
    "trainset = trainset.merge(pcode_per_day, on=['day_order', 'pgm_order', 'pcode'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAR 7) 이전 판매량 관련 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_pre_median = []; vol_pre_min = []; vol_pre_max = []; vol_pre_var = []\n",
    "for d, p in trainset[['day_order', 'pcode']].values :\n",
    "    temp1 = trainset.loc[(trainset.pcode == p) & (trainset.day_order < d),:].sales_volume.values\n",
    "    check = temp1.tolist()\n",
    "    if check :\n",
    "        vol_pre_median.append(np.median(temp1))\n",
    "        vol_pre_max.append(temp1.max())\n",
    "        vol_pre_min.append(temp1.min())\n",
    "        vol_pre_var.append(temp1.var())\n",
    "    else :\n",
    "        vol_pre_median.append(None)\n",
    "        vol_pre_max.append(None)\n",
    "        vol_pre_min.append(None)\n",
    "        vol_pre_var.append(None)\n",
    "\n",
    "trainset['pre_volume_median'] = vol_pre_median\n",
    "trainset['pre_volume_max'] = vol_pre_max\n",
    "trainset['pre_volume_min'] = vol_pre_min\n",
    "trainset['pre_volume_var'] = vol_pre_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAR 8) 이전 취급액 관련 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_pre_median = []; sales_pre_min = []; sales_pre_max = []; sales_pre_var = []\n",
    "for d, p in trainset[['day_order', 'pcode']].values :\n",
    "    temp1 = trainset.loc[(trainset.pcode == p) & (trainset.day_order < d),:].sales.values\n",
    "    check = temp1.tolist()\n",
    "    if check :\n",
    "        sales_pre_median.append(np.median(temp1))\n",
    "        sales_pre_max.append(temp1.max())\n",
    "        sales_pre_min.append(temp1.min())\n",
    "        sales_pre_var.append(temp1.var())\n",
    "    else :\n",
    "        sales_pre_median.append(None)\n",
    "        sales_pre_max.append(None)\n",
    "        sales_pre_min.append(None)\n",
    "        sales_pre_var.append(None)\n",
    "\n",
    "trainset['pre_sales_median'] = sales_pre_median\n",
    "trainset['pre_sales_max'] = sales_pre_max\n",
    "trainset['pre_sales_min'] = sales_pre_min\n",
    "trainset['pre_sales_var'] = sales_pre_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAR 9) 상품 당 duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset['dur_per_p'] = trainset.duration / trainset.p_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAR 10) 세일 폭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = trainset.groupby('pname').price.unique()\n",
    "s = s[s.apply(lambda x : len(x) > 1)]\n",
    "\n",
    "def myfunc(df) :\n",
    "    max_sales = df.price.max()\n",
    "    promotion_ratio = (max_sales - df.price)/max_sales\n",
    "    price = df.price\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'pname' : df.pname,\n",
    "        'price' : price,\n",
    "        'promotion_ratio' : promotion_ratio\n",
    "    })\n",
    "\n",
    "상품가격할인율 = trainset.groupby(['pname']).apply(myfunc)\n",
    "ratio_dict = 상품가격할인율.drop_duplicates(['pname', 'price']).set_index(['pname', 'price'])\n",
    "\n",
    "tmp = []\n",
    "for i,r in trainset.iterrows() :\n",
    "    tmp.append(ratio_dict.loc[(r['pname'], r['price'])][0])\n",
    "\n",
    "trainset['promotion_ratio'] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAR 11) 외부변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.rename(columns={\"hm\": \"humid\"}, inplace=True)\n",
    "\n",
    "weather = weather.rename(columns={'tm':'datetime'})\n",
    "munzi.columns = ['stdid', 'stdnm', 'datetime', 'munzi']\n",
    "wehter_g = weather.groupby('datetime').mean().reset_index()[['datetime', 'ta', 'rn', 'ws', 'humid', 'ss', 'icsr', 'dsnw']]\n",
    "munzi_g = munzi.groupby('datetime').mean().reset_index()[['datetime', 'munzi']]\n",
    "\n",
    "wehter_g['datetime'] = pd.to_datetime(wehter_g.datetime)\n",
    "munzi_g['datetime'] = pd.to_datetime(munzi_g.datetime)\n",
    "train_dt = pd.DataFrame({'datetime':trainset.datetime.dt.strftime('%Y-%m-%d %H:00:00')})\n",
    "train_dt['datetime'] = pd.to_datetime(train_dt.datetime)\n",
    "\n",
    "weather_features = train_dt.merge(wehter_g, on='datetime', how='left')\n",
    "munzi_features = train_dt.merge(munzi_g, on='datetime', how='left')\n",
    "weather_features['munzi'] = munzi_features['munzi']\n",
    "\n",
    "del weather_features['ss']; del weather_features['icsr']\n",
    "weather_features['rn'] = [1 if i > 0 else 0 for i in weather_features['rn']]\n",
    "weather_features['dsnw'] = [1 if i > 0 else 0 for i in weather_features['dsnw']]\n",
    "trainset = pd.concat([trainset, weather_features.iloc[:,1:]], axis=1)\n",
    "\n",
    "\n",
    "# 시청률 변수 생성\n",
    "watching_rate = watching_rate_data.set_index(watching_rate_data['시간대']).iloc[:-1,:-1]\n",
    "watching_rate = watching_rate.iloc[:,1:].unstack().reset_index()\n",
    "watching_rate.columns = ['date', 'time', 'w_rate']\n",
    "watching_rate['datetime'] = watching_rate.date+' '+watching_rate.time\n",
    "del watching_rate['date']; del watching_rate['time']\n",
    "watching_rate = watching_rate[['datetime', 'w_rate']]\n",
    "\n",
    "watching_rate['datetime'] = pd.to_datetime(watching_rate['datetime'])\n",
    "\n",
    "\n",
    "time_index = pd.DataFrame({'start':pd.to_datetime(pd.Series(trainset.datetime.unique().astype('str').tolist())), \n",
    "                           'end':pd.to_datetime(pd.Series(trainset.datetime.unique()[1:].astype('str').tolist() + ['2020-01-01T00:20:00.000000000']))})\n",
    "\n",
    "wr_sum = [] # 한 duration 내 총 시청률 합계\n",
    "for row in time_index.values :\n",
    "    wr_sum.append(watching_rate.loc[(watching_rate.datetime >= row[0]) & (watching_rate.datetime < row[1]),:].w_rate.sum())\n",
    "time_index['watching_rate'] = pd.Series(wr_sum)\n",
    "del time_index['end']\n",
    "time_index.columns = ['datetime', 'watching_rate']\n",
    "\n",
    "# -- 시청률 변수 추가\n",
    "trainset = trainset.merge(time_index, on='datetime', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAR 12) 상품명 토큰변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = {'token0': '10인용', 'token1': '6인용', 'token2': '4인용', 'token3': '3인용', \n",
    "              'token4': '6형', 'token5': '7형', \n",
    "              'token6': '75UK', 'token7': '70UK', 'token8': '65UK', 'token9': '55UK', 'token10': 'FQ17', 'token11': 'FQ19',\n",
    "              'token12': '스탠드형', 'token13': '2IN1형',\n",
    "              'token14': 'S10JK', 'token15': 'S06JK',\n",
    "              'token16': '킹', 'token17': '퀸', 'token18': '슈퍼싱글',\n",
    "              'token19': 'EV 040', 'token20': 'EV 020',\n",
    "              'token21': '16R5773WSR', 'token22': '16R5773WSK',\n",
    "              'token23': '홈멀티', 'token24': '스탠드',\n",
    "              'token25': '아동',\n",
    "              'token26': '2구', 'token27': '3구'}\n",
    "\n",
    "for k, v in token_dict.items() :\n",
    "    trainset[k] = pd.Series()\n",
    "    temp = []\n",
    "    for p in trainset.pname :\n",
    "        if v in p :\n",
    "            temp.append(1)\n",
    "        else :\n",
    "            temp.append(0)\n",
    "    trainset[k] = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAR 13) SALES_0 에 등장한 적 있는지의 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_0_before = []\n",
    "\n",
    "for i in trainset.pcode :\n",
    "    if i in sales_0_df['pcode'].values :\n",
    "        sales_0_before.append(1)\n",
    "    else :\n",
    "        sales_0_before.append(0)\n",
    "\n",
    "trainset['sales_0_before'] = sales_0_before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAR 14) 남성상품/여성상품/해당없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = re.compile('(남성)|(여성)')\n",
    "pat_male = re.compile('남성')\n",
    "pat_female = re.compile('여성')\n",
    "\n",
    "성별_리스트 = []\n",
    "for 상품명 in trainset.pname :\n",
    "    if '남성' in 상품명 : \n",
    "        성별_리스트.append(1)\n",
    "    elif '여성' in 상품명 : \n",
    "        성별_리스트.append(2)\n",
    "    else :\n",
    "        성별_리스트.append(0)\n",
    "\n",
    "trainset['gender'] = 성별_리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAR 15) 결제 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "무이자일시불 = []\n",
    "무이자표현 = ['무이자', '무)']\n",
    "일시불표현 = ['일시불', '일)']\n",
    "for i in trainset.pname :\n",
    "    초기값 = 0\n",
    "    for j in 무이자표현 :\n",
    "        if j in i :\n",
    "            초기값 = 1 # 무이자\n",
    "    for j in 일시불표현 :\n",
    "        if j in i :\n",
    "            초기값 = 2 # 일시불\n",
    "    무이자일시불.append(초기값)\n",
    "\n",
    "trainset['payment_opt'] = 무이자일시불"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAR 17) search(상품 검색량), trend(사회적 계절성) 변수  추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_df.rename(columns={'날짜':'datetime'}, inplace=True)\n",
    "trend_dict.columns = ['key', 'trend']\n",
    "trend_df['datetime'] = pd.to_datetime(trend_df['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = pd.concat([trainset,\n",
    "                      pd.DataFrame({'datetime':pd.to_datetime(trainset.datetime.dt.date)}).merge(trend_df, on='datetime', how='left').iloc[:,1:]], \n",
    "                     axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_df.rename(columns={'날짜':'datetime'}, inplace=True)\n",
    "search_dict.columns = ['key', 'trend']\n",
    "search_df['datetime'] = pd.to_datetime(search_df['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = pd.concat([trainset,\n",
    "                      pd.DataFrame({'datetime':pd.to_datetime(trainset.datetime.dt.date)}).merge(search_df, on='datetime', how='left').iloc[:,1:]], \n",
    "                     axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAR 18) 이전 판매기록의 변수화\n",
    "- 판매량이 5보다 작았던 적이 있는 pcode이다 / 판매량이 10 보다 작았던 적이 있는 pcode 이다\n",
    "- 취급액이 100만원보다 작았던 적이 있는 pcode이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_under5_pcode = trainset[trainset.sales_volume < 6].pcode.unique().tolist()\n",
    "vol_under10_pcode = trainset[trainset.sales_volume < 11].pcode.unique().tolist()\n",
    "sales_under100_pcode = trainset[trainset.sales < 1000000].pcode.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset['vol_under5'] = pd.Series([1 if i in vol_under5_pcode else 0 for i in trainset.pcode])\n",
    "trainset['vol_under10'] = pd.Series([1 if i in vol_under10_pcode else 0 for i in trainset.pcode])\n",
    "trainset['sales_under100'] = pd.Series([1 if i in sales_under100_pcode else 0 for i in trainset.pcode])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAR 19) 상품별 재방송(repeat_order)에 따른 증가율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_ratio = []\n",
    "product_pgm_index = trainset.groupby(['pcode', 'day_order', 'pgm_order']).count().reset_index()[['pcode', 'day_order', 'pgm_order']]\n",
    "for c, d, p in product_pgm_index.values :\n",
    "    temp = trainset.loc[(trainset.pcode == c) & (trainset.day_order == d) & (trainset.pgm_order == p) , :]\n",
    "    up_ratio.append(temp.sales.tolist()[-1] / temp.sales.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_pgm_index['up_ratio'] = up_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_ratio_median = []; repeat_ratio_max = []; repeat_ratio_min = []; repeat_ratio_var = []\n",
    "for values in trainset[['pcode', 'day_order' ,'pgm_order']].values :\n",
    "    temp = product_pgm_index.loc[(product_pgm_index.pcode == values[0]) & (product_pgm_index.day_order <= values[0]), :]\n",
    "    repeat_ratio_median.append(np.median(temp.up_ratio)); repeat_ratio_max.append(max(temp.up_ratio))\n",
    "    repeat_ratio_min.append(min(temp.up_ratio)); repeat_ratio_var.append(np.var(temp.up_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset['repeat_ratio_median'] = repeat_ratio_median\n",
    "trainset['repeat_ratio_max'] = repeat_ratio_max\n",
    "trainset['repeat_ratio_min'] = repeat_ratio_min\n",
    "trainset['repeat_ratio_var'] = repeat_ratio_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAR 20) 브랜드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_df.columns = ['pname', 'brand', '']\n",
    "brand_df = brand_df.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = trainset.merge(brand_df, on='pname', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_dict = {b:l for b, l in zip(trainset.brand, LabelEncoder().fit_transform(trainset.brand))}\n",
    "trainset['brand'] = LabelEncoder().fit_transform(trainset.brand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  preprocessing(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리 => 한글\n",
    "cate_dict = {'의류':'clothes', \n",
    "             '속옷':'under_wear', \n",
    "             '주방':'kitchen', \n",
    "             '농수축':'argo_farming', \n",
    "             '이미용':'beauty', \n",
    "             '가전':'electronics', \n",
    "             '생활용품':'living', \n",
    "             '건강기능':'health', \n",
    "             '잡화':'general_merchandise', \n",
    "             '가구':'furnitures', \n",
    "             '침구':'beds'}\n",
    "trainset = trainset.replace({\"category\": cate_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duration 구간화\n",
    "trainset['duration'] = trainset['duration'].round(-1)\n",
    "trainset['duration'].replace({0:20, 40:30}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Modeling\n",
    "#### - 유사코드 부여를 위한 trainset 상품명 vector화 / 유사코드 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pname_bigram_train_df, pname_voab = get_pname_token_df(trainset, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_mcode_model, pseudo_pcode_model, train_mcode_cols, train_pcode_cols, \\\n",
    "train_mcodes_uniques, train_pcodes_uniques = make_pseudo_m_pcode(trainset, pname_bigram_train_df,\n",
    "                                                                 train=True, \n",
    "                                                                 train_validation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Make Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = preprocessing(testset, train=False)\n",
    "testset = date_split(testset, train=False)\n",
    "testset = day_pgm_repeat_order(testset, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_order = []\n",
    "for i in testset.day :\n",
    "    if 1 <= i <= 7 :\n",
    "        week_order.append(1)\n",
    "    elif 8 <= i <= 15 :\n",
    "        week_order.append(2)\n",
    "    elif 16 <= i <= 23 :\n",
    "        week_order.append(3)\n",
    "    else :\n",
    "        week_order.append(4)\n",
    "testset['week_order'] = pd.Series(week_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday = []\n",
    "for i, j in zip(testset.datetime.dt.date, testset.weekday) :\n",
    "    if j in ('sat', 'sun') :\n",
    "        holiday.append(1)\n",
    "    else :\n",
    "        holiday.append(0)\n",
    "testset['holiday'] = holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_nunique = testset.groupby(['day_order', 'pgm_order']).nunique().price.reset_index()\n",
    "price_nunique = price_nunique[price_nunique.price > 1].reset_index(drop=True)\n",
    "\n",
    "use_nan = None; high_price = []; low_price = []\n",
    "cols = testset.columns.tolist()\n",
    "for row in testset.values :\n",
    "    if price_nunique.loc[(price_nunique.day_order == row[cols.index('day_order')]) \n",
    "                         & (price_nunique.pgm_order == row[cols.index('pgm_order')]), :].shape[0] :\n",
    "        temp = testset.loc[(testset.day_order == row[cols.index('day_order')]) \n",
    "                           & (testset.pgm_order == row[cols.index('pgm_order')]), :]\n",
    "        if row[cols.index('price')] == temp.price.max() :\n",
    "            high_price.append(1)\n",
    "        else :\n",
    "            high_price.append(0)\n",
    "        if row[cols.index('price')] == temp.price.min() :\n",
    "            low_price.append(1)\n",
    "        else :\n",
    "            low_price.append(0)            \n",
    "    else :\n",
    "        high_price.append(2) # 해당 없음\n",
    "        low_price.append(2) # 해당 없음\n",
    "\n",
    "testset['high_price'] = pd.Series(high_price)\n",
    "testset['low_price'] = pd.Series(low_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_bin = []\n",
    "for i in testset.price :\n",
    "    for q, l in qcut_idx_df.values :\n",
    "        if i in q :\n",
    "            price_bin.append(l)\n",
    "            break\n",
    "    else :\n",
    "        if i <= q.left :\n",
    "            price_bin.append(0)\n",
    "        elif i > q.right :\n",
    "            price_bin.append(9)\n",
    "testset['price_bin'] = price_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = testset.groupby('datetime').count().pcode.reset_index()\n",
    "temp_df.columns = ['datetime', 'p_cnt']\n",
    "testset = testset.merge(temp_df, on=['datetime'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = testset.merge(brand_df, on='pname', how='left')\n",
    "testset['brand'] = testset['brand'].replace(brand_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 유사 마더코드/상품코드 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pname_bigram_test_df = get_pname_token_df(testset, \n",
    "                                          pname_cols=pname_bigram_train_df.columns.tolist(), \n",
    "                                          pname_vocab=pname_voab, \n",
    "                                          train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testset 구성 중..\n",
      "유사코드 생성 중..\n"
     ]
    }
   ],
   "source": [
    "testset, mp_code_raw, pseudo_mcode_df, pseudo_pcode_df = make_pseudo_m_pcode(testset, \n",
    "                                                                             pname_bigram_test_df, \n",
    "                                                                             pseudo_mcode_model=pseudo_mcode_model, \n",
    "                                                                             pseudo_pcode_model=pseudo_pcode_model, \n",
    "                                                                             m_cols=train_mcode_cols, \n",
    "                                                                             p_cols=train_pcode_cols, \n",
    "                                                                             train=False, \n",
    "                                                                             train_validation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset에 없는 mcode, pcode의 경우, pseudo code 사전을 참조하여 대체\n",
    "\n",
    "pseudo_mcode = []\n",
    "for i in testset.mcode :\n",
    "    if i in train_mcodes_uniques :\n",
    "        pseudo_mcode.append(i)\n",
    "    else :\n",
    "        pseudo_mcode.append(pseudo_mcode_df[pseudo_mcode_df.real==i].pseudo.unique().tolist()[0])\n",
    "        \n",
    "pseudo_pcode = []\n",
    "for i in testset.pcode :\n",
    "    if i in train_pcodes_uniques :\n",
    "        pseudo_pcode.append(i)\n",
    "    else :\n",
    "        pseudo_pcode.append(pseudo_pcode_df[pseudo_pcode_df.real==i].pseudo.unique().tolist()[0])\n",
    "\n",
    "testset['mcode'] = pseudo_mcode\n",
    "testset['pcode'] = pseudo_pcode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - accumed_cnt/fist_sale (누적 방송 횟수/첫 방송 여부)\n",
    "유사코드 대체 후, 해당 코드의 방송 중 가장 마지막 방송의 값을 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumed_cnt = []; first_sale = []\n",
    "for p in testset.pcode :\n",
    "    temp = trainset.loc[(trainset.pcode==p)]\n",
    "    accumed_cnt.append(temp.accumed_cnt.values.tolist()[-1])\n",
    "    first_sale.append(temp.first_sale.values.tolist()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset['accumed_cnt'] = pd.Series(accumed_cnt)\n",
    "testset['first_sale'] = pd.Series(first_sale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - 이전 판매량 / 이전 취급액 / 이전 판매기록 관련 변수\n",
    "유사코드 대체 후, 해당 코드의 방송 중 가장 마지막 방송의 값을 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 판매기록 관련 리스트\n",
    "sales_0_before = []; vol_under5 = []; vol_under10 = []; sales_under100 = []\n",
    "for i in testset.pcode :\n",
    "    if i in sales_0_pcodes :\n",
    "        sales_0_before.append(1)\n",
    "    else :\n",
    "        sales_0_before.append(0)\n",
    "    if i in vol_under5_pcode :\n",
    "        vol_under5.append(1)\n",
    "    else :\n",
    "        vol_under5.append(0)\n",
    "    if i in vol_under10_pcode :\n",
    "        vol_under10.append(1)\n",
    "    else :\n",
    "        vol_under10.append(0)\n",
    "    if i in sales_under100_pcode :\n",
    "        sales_under100.append(1)\n",
    "    else :\n",
    "        sales_under100.append(0)\n",
    "\n",
    "repeat_cols = ['repeat_ratio_median', 'repeat_ratio_max', 'repeat_ratio_min', 'repeat_ratio_var']\n",
    "pre_cols = ['pre_volume_median', 'pre_volume_max', 'pre_volume_min', 'pre_volume_var',\n",
    "            'pre_sales_median', 'pre_sales_max', 'pre_sales_min', 'pre_sales_var']\n",
    "\n",
    "repeat_ratio_median = []; repeat_ratio_max = []; repeat_ratio_min = []; repeat_ratio_var = []\n",
    "pre_volume_median = []; pre_volume_max = []; pre_volume_min = []; pre_volume_var = []\n",
    "pre_sales_median = []; pre_sales_max = []; pre_sales_min = []; pre_sales_var = []\n",
    "\n",
    "for p in testset.pcode :\n",
    "    temp = trainset.loc[(trainset.pcode==p)]\n",
    "    repeat_ratio_median.append(temp.repeat_ratio_median.values.tolist()[-1])\n",
    "    repeat_ratio_max.append(temp.repeat_ratio_max.values.tolist()[-1])\n",
    "    repeat_ratio_min.append(temp.repeat_ratio_min.values.tolist()[-1])\n",
    "    repeat_ratio_var.append(temp.repeat_ratio_var.values.tolist()[-1])\n",
    "    pre_volume_median.append(temp.pre_volume_median.values.tolist()[-1])\n",
    "    pre_volume_max.append(temp.pre_volume_max.values.tolist()[-1])\n",
    "    pre_volume_min.append(temp.pre_volume_min.values.tolist()[-1])\n",
    "    pre_volume_var.append(temp.pre_volume_var.values.tolist()[-1])\n",
    "    pre_sales_median.append(temp.pre_sales_median.values.tolist()[-1])\n",
    "    pre_sales_max.append(temp.pre_sales_max.values.tolist()[-1])\n",
    "    pre_sales_min.append(temp.pre_sales_min.values.tolist()[-1])\n",
    "    pre_sales_var.append(temp.pre_sales_var.values.tolist()[-1])\n",
    "\n",
    "testset['sales_0_before'] = sales_0_before\n",
    "testset['vol_under5'] = vol_under5\n",
    "testset['vol_under10'] = vol_under10\n",
    "testset['sales_under100'] = sales_under100\n",
    "testset['repeat_ratio_median'] = repeat_ratio_median\n",
    "testset['repeat_ratio_max'] = repeat_ratio_max\n",
    "testset['repeat_ratio_min'] = repeat_ratio_min\n",
    "testset['repeat_ratio_var'] = repeat_ratio_var\n",
    "testset['pre_volume_median'] = pre_volume_median\n",
    "testset['pre_volume_max'] = pre_volume_max\n",
    "testset['pre_volume_min'] = pre_volume_min\n",
    "testset['pre_volume_var'] = pre_volume_var\n",
    "testset['pre_sales_median'] = pre_sales_median\n",
    "testset['pre_sales_max'] = pre_sales_max\n",
    "testset['pre_sales_min'] = pre_sales_min\n",
    "testset['pre_sales_var'] = pre_sales_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset['dur_per_p'] = testset.duration / testset.p_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = testset.groupby('pname').price.unique()\n",
    "s = s[s.apply(lambda x : len(x) > 1)]\n",
    "\n",
    "def myfunc(df) :\n",
    "    max_sales = df.price.max()\n",
    "    promotion_ratio = (max_sales - df.price)/max_sales\n",
    "    price = df.price\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'pname' : df.pname,\n",
    "        'price' : price,\n",
    "        'promotion_ratio' : promotion_ratio\n",
    "    })\n",
    "\n",
    "상품가격할인율 = testset.groupby(['pname']).apply(myfunc)\n",
    "ratio_dict = 상품가격할인율.drop_duplicates(['pname', 'price']).set_index(['pname', 'price'])\n",
    "\n",
    "tmp = []\n",
    "for i, r in testset.iterrows() :\n",
    "    tmp.append(ratio_dict.loc[(r['pname'], r['price'])][0])\n",
    "\n",
    "testset['promotion_ratio'] = tmp\n",
    "testset['promotion_ratio'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "munzi = pd.read_csv('./data/external_data/weather/munzi_pm10_utf.csv')\n",
    "brand_df = pd.read_excel('./data/external_data/product_info/brand.xlsx').iloc[:,1:]\n",
    "\n",
    "weather.rename(columns={\"hm\": \"humid\"}, inplace=True)\n",
    "\n",
    "weather = weather.rename(columns={'tm':'datetime'})\n",
    "munzi.columns = ['stdid', 'stdnm', 'datetime', 'munzi']\n",
    "wehter_g = weather.groupby('datetime').mean().reset_index()[['datetime', 'ta', 'rn', 'ws', 'humid', 'ss', 'icsr', 'dsnw']]\n",
    "munzi_g = munzi.groupby('datetime').mean().reset_index()[['datetime', 'munzi']]\n",
    "\n",
    "wehter_g['datetime'] = pd.to_datetime(wehter_g.datetime)\n",
    "munzi_g['datetime'] = pd.to_datetime(munzi_g.datetime)\n",
    "test_dt = pd.DataFrame({'datetime':testset.datetime.dt.strftime('%Y-%m-%d %H:00:00')})\n",
    "test_dt['datetime'] = pd.to_datetime(test_dt.datetime)\n",
    "\n",
    "weather_features = test_dt.merge(wehter_g, on='datetime', how='left')\n",
    "munzi_features = test_dt.merge(munzi_g, on='datetime', how='left')\n",
    "weather_features['munzi'] = munzi_features['munzi']\n",
    "\n",
    "del weather_features['ss']; del weather_features['icsr']\n",
    "weather_features['rn'] = [1 if i > 0 else 0 for i in weather_features['rn']]\n",
    "weather_features['dsnw'] = [1 if i > 0 else 0 for i in weather_features['dsnw']]\n",
    "testset = pd.concat([testset, weather_features.iloc[:,1:]], axis=1)\n",
    "\n",
    "\n",
    "# 시청률 변수 생성\n",
    "watching_rate = watching_rate_data.set_index(watching_rate_data['시간대']).iloc[:-1,:-1]\n",
    "watching_rate = watching_rate.iloc[:,1:].unstack().reset_index()\n",
    "watching_rate.columns = ['date', 'time', 'w_rate']\n",
    "watching_rate['datetime'] = watching_rate.date+' '+watching_rate.time\n",
    "del watching_rate['date']; del watching_rate['time']\n",
    "watching_rate = watching_rate[['datetime', 'w_rate']]\n",
    "\n",
    "watching_rate['datetime'] = watching_rate['datetime'].str.replace('2019', '2020')    \n",
    "watching_rate['datetime'] = pd.to_datetime(watching_rate['datetime'])\n",
    "time_index = pd.DataFrame({'start':pd.to_datetime(pd.Series(testset.datetime.unique().astype('str').tolist())), \n",
    "                           'end':pd.to_datetime(pd.Series(testset.datetime.unique()[1:].astype('str').tolist() + ['2020-07-01T00:20:00.000000000']))})        \n",
    "\n",
    "wr_sum = [] # 한 duration 내 총 시청률 합계\n",
    "for row in time_index.values :\n",
    "    wr_sum.append(watching_rate.loc[(watching_rate.datetime >= row[0]) & (watching_rate.datetime < row[1]),:].w_rate.sum())\n",
    "time_index['watching_rate'] = pd.Series(wr_sum)\n",
    "del time_index['end']\n",
    "time_index.columns = ['datetime', 'watching_rate']\n",
    "\n",
    "# -- 시청률 변수 추가\n",
    "testset = testset.merge(time_index, on='datetime', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in token_dict.items() :\n",
    "    testset[k] = pd.Series()\n",
    "    temp = []\n",
    "    for p in testset.pname :\n",
    "        if v in p :\n",
    "            temp.append(1)\n",
    "        else :\n",
    "            temp.append(0)\n",
    "    testset[k] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = re.compile('(남성)|(여성)')\n",
    "pat_male = re.compile('남성')\n",
    "pat_female = re.compile('여성')\n",
    "\n",
    "성별_리스트 = []\n",
    "for 상품명 in testset.pname :\n",
    "    if '남성' in 상품명 : \n",
    "        성별_리스트.append(1)\n",
    "    elif '여성' in 상품명 : \n",
    "        성별_리스트.append(2)\n",
    "    else :\n",
    "        성별_리스트.append(0)\n",
    "\n",
    "testset['gender'] = 성별_리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "무이자일시불 = []\n",
    "무이자표현 = ['무이자', '무)']\n",
    "일시불표현 = ['일시불', '일)']\n",
    "for i in testset.pname :\n",
    "    초기값 = 0\n",
    "    for j in 무이자표현 :\n",
    "        if j in i :\n",
    "            초기값 = 1 # 무이자\n",
    "    for j in 일시불표현 :\n",
    "        if j in i :\n",
    "            초기값 = 2 # 일시불\n",
    "    무이자일시불.append(초기값)\n",
    "\n",
    "testset['payment_opt'] = 무이자일시불"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = pd.concat([testset,\n",
    "                      pd.DataFrame({'datetime':pd.to_datetime(testset.datetime.dt.date)}).merge(trend_df, on='datetime', how='left').iloc[:,1:]], \n",
    "                     axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = pd.concat([testset,\n",
    "                      pd.DataFrame({'datetime':pd.to_datetime(testset.datetime.dt.date)}).merge(search_df, on='datetime', how='left').iloc[:,1:]], \n",
    "                     axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리 => 한글\n",
    "cate_dict = {'의류':'clothes', \n",
    "             '속옷':'under_wear', \n",
    "             '주방':'kitchen', \n",
    "             '농수축':'argo_farming', \n",
    "             '이미용':'beauty', \n",
    "             '가전':'electronics', \n",
    "             '생활용품':'living', \n",
    "             '건강기능':'health', \n",
    "             '잡화':'general_merchandise', \n",
    "             '가구':'furnitures', \n",
    "             '침구':'beds', \n",
    "             '무형':'mu'}\n",
    "testset = testset.replace({\"category\": cate_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duration 구간화\n",
    "testset['duration'] = testset['duration'].round(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - make models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - make X features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_cols = ['month', 'hour', 'minute', 'weekday', 'duration', 'mcode', 'pcode', 'category', \n",
    "              'pgm_order', 'repeat_order', 'week_order', 'holiday', 'high_price', 'low_price', 'price_bin']\n",
    "numeric_binary_cols = ['p_cnt', 'accumed_cnt', 'first_sale', 'price',\n",
    "                       'pre_volume_median', 'pre_volume_max', 'pre_volume_min', 'pre_volume_var', \n",
    "                       'pre_sales_median', 'pre_sales_max', 'pre_sales_min', 'pre_sales_var', \n",
    "                       'dur_per_p', 'promotion_ratio', \n",
    "                       'ta', 'rn', 'ws', 'humid', 'dsnw', 'munzi', 'watching_rate', \n",
    "                       'token0', 'token1', 'token2', 'token3', 'token4', 'token5', 'token6', 'token7', 'token8', \n",
    "                       'token9', 'token10', 'token11', 'token12', 'token13', 'token14', 'token15', 'token16', \n",
    "                       'token17', 'token18', 'token19', 'token20', 'token21', 'token22', 'token23', 'token24', \n",
    "                       'token25', 'token26', 'token27', 'sales_0_before', 'gender', 'payment_opt', \n",
    "                       'trend0', 'trend1', 'trend2', 'trend3', 'trend4', 'trend5', 'trend6', 'trend7', 'trend8', \n",
    "                       'search0', 'search1', 'search2', 'search3', 'search4', 'search5', 'search6', 'search7', 'search8', \n",
    "                       'search9', 'search10', 'search11', 'search12', 'search13', 'search14', 'search15', 'search16', 'search17', \n",
    "                       'search18', 'search19', 'search20', 'search21', 'search22', 'search23', 'search24', 'search25', 'search26', \n",
    "                       'search27', 'search28', 'search29', 'vol_under5', 'vol_under10', 'sales_under100', \n",
    "                       'repeat_ratio_median', 'repeat_ratio_max', 'repeat_ratio_min', 'repeat_ratio_var']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df = pd.DataFrame()\n",
    "for col in dummy_cols :\n",
    "    dummy_df[col] = pd.Series()\n",
    "    dummy_df[col] = trainset[col].astype('str')\n",
    "dummy_df = pd.get_dummies(dummy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = pd.concat([dummy_df, trainset[numeric_binary_cols]], axis=1)\n",
    "Y = trainset.sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df_ = pd.DataFrame()\n",
    "for col in dummy_cols :\n",
    "    dummy_df_[col] = pd.Series()\n",
    "    dummy_df_[col] = testset[col].astype('str')\n",
    "dummy_df_ = pd.get_dummies(dummy_df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test_temp = pd.concat([dummy_df_, testset[numeric_binary_cols]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_cols = features_train.columns.tolist()\n",
    "tt_cols = features_test_temp.columns.tolist()\n",
    "features_test = pd.DataFrame(columns = tr_cols)\n",
    "\n",
    "for c in tr_cols :\n",
    "    if c in tt_cols :\n",
    "        features_test[c] = features_test_temp[c]\n",
    "    else :\n",
    "        features_test[c] = pd.Series([0 for i in range(features_test_temp.shape[0])])\n",
    "\n",
    "features_test = features_test[tr_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - get predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    'task': 'train',\n",
    "    'num_iterations':1500,\n",
    "    'boosting_type': 'dart',\n",
    "    'max_depth':11,\n",
    "    'num_leaves':2023,\n",
    "    'drop_rate' : 0.5,\n",
    "    'objective': 'regression_l1',\n",
    "    'tree_learner':'voting',\n",
    "    'num_threads': -1,\n",
    "    'metric': [mape],\n",
    "    'feature_fraction':0.8,\n",
    "    'learning_rate': 0.01\n",
    "    }\n",
    "\n",
    "\n",
    "train_data = Dataset(features_train.values, Y.values)\n",
    "model = lightgbm.train(hyper_params, \n",
    "                       train_set=train_data,\n",
    "                       verbose_eval=True)\n",
    "\n",
    "pred = model.predict(features_test.values)\n",
    "pred = np.array([features_test.price[i] if p < 0 else p for i, p in enumerate(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_to_submit = pd.read_excel('./data/02_평가데이터/2020 빅콘테스트 데이터분석분야-챔피언리그_2020년 6월 판매실적예측데이터(평가데이터).xlsx', skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_to_submit['취급액'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_to_submit['취급액'] = pd.Series([None if p == '무형' else s for p, s in zip(testset_to_submit['상품군'], testset_to_submit['취급액'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_to_submit.to_csv('predict_result_.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
